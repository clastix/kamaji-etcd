# Default values for kamaji-crane.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Size of the etcd cluster
replicas: 3

# -- Install an etcd with enabled multi-tenancy
serviceAccount:
  # -- Create a ServiceAccount, required to install and provision the etcd backing storage (default: true)
  create: true
  # -- Define the ServiceAccount name to use during the setup and provision of the etcd backing storage (default: "")
  name: ""

image:
  # -- Install image from specific repo 
  repository: quay.io/coreos/etcd
  # -- Install image with specific tag, overwrite the tag in the chart
  tag: ""
  # -- Pull policy to use
  pullPolicy: IfNotPresent

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# -- The peer API port which servers are listening to.
peerApiPort: 2380

# -- The client request port.
clientPort: 2379

# -- The port where etcd exposes metrics.
metricsPort: 2381

# -- The livenessProbe for the etcd container
livenessProbe:
  failureThreshold: 3
  httpGet:
    path: /livez
    port: 2381
    scheme: HTTP
  initialDelaySeconds: 10
  periodSeconds: 10
  timeoutSeconds: 15

# -- Domain of the Kubernetes cluster.
clusterDomain: "cluster.local"

# -- A list of extra arguments to add to the etcd default ones
extraArgs: []
#- --log-level=warn
#- --logger=zap

# -- Interpret 'auto-compaction-retention' one of: periodic|revision. Use 'periodic' for duration based retention, 'revision' for revision number based retention.
autoCompactionMode: periodic

# -- Auto compaction retention length. 0 means disable auto compaction.
autoCompactionRetention: 5m

# -- Number of committed transactions to trigger a snapshot to disk.
snapshotCount: "10000"

# -- Raise alarms when backend size exceeds the given quota. It will put the cluster into a maintenance mode which only accepts key reads and deletes. 
quotaBackendBytes: "8589934592" # 8Gi

persistentVolumeClaim:
  # -- The size of persistent storage for etcd data 
  size: 8Gi
  # -- A specific storage class
  storageClassName: ""
  # -- The Access Mode to storage
  accessModes:
  - ReadWriteOnce
  # -- The custom annotations to add to the PVC
  customAnnotations: {}
  #  volumeType: local

# -- Labels to add to all etcd pods
podLabels:
  application: kamaji-etcd

# -- Annotations to add to all etcd pods
podAnnotations: {}

# -- The securityContext to apply to etcd
securityContext:
  allowPrivilegeEscalation: false

# -- The priorityClassName to apply to etcd
priorityClassName: system-cluster-critical

# -- Resources assigned to the etcd containers
resources:
  limits: {}
  requests: {}

# -- Kubernetes node selector rules to schedule etcd
nodeSelector:
  kubernetes.io/os: linux

# -- Kubernetes node taints that the etcd pods would tolerate
tolerations: []

# -- Kubernetes affinity rules to apply to etcd pods
affinity: {}
  # podAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app.kubernetes.io/instance
  #         operator: In
  #         values:
  #         -  kamaji-etcd
  #     topologyKey: "kubernetes.io/hostname"

# -- Kubernetes topology spread constraints to apply to etcd pods
topologySpreadConstraints: []
#- maxSkew: 1
#  topologyKey: topology.kubernetes.io/zone
#  whenUnsatisfiable: DoNotSchedule
#  labelSelector:
#    matchLabels:
#      application: kamaji-etcd

datastore:
  # -- Create a datastore custom resource for Kamaji
  enabled: true
  # -- Name of Kamaji datastore, set to fully qualified etcd name when null or not provided
  name: ""
  # -- Expose the headless service endpoints in the datastore. Set to false to expose with regular service.
  headless: true # https://github.com/clastix/kamaji/issues/856
  # -- Assign additional Annotations to the datastore
  annotations: {}
  #  helm.sh/resource-policy: keep

serviceMonitor:
  # -- Enable ServiceMonitor for Prometheus
  enabled: false
  # -- Install the ServiceMonitor into a different namespace than release one.
  namespace: ''
  # -- Assign additional labels according to Prometheus' serviceMonitorSelector matching labels. By default, it uses the kube-prometheus-stack one.
  labels:
    release: kube-prometheus-stack
  # -- Assign additional Annotations
  annotations: {}
  # -- Change matching labels. By default, it uses client service labels.
  matchLabels: {}
  # -- Set targetLabels for the serviceMonitor
  targetLabels: []
  # -- ServiceAccount for scraping metrics from etcd. By defult, it uses the kube-prometheus-stack one.
  serviceAccount:
    # -- ServiceAccount name
    name: kube-prometheus-stack-prometheus
    # -- ServiceAccount namespace
    namespace: monitoring-system
  endpoint:
    # -- Set the scrape interval for the endpoint of the serviceMonitor
    interval: "15s"
    # -- Set the scrape timeout for the endpoint of the serviceMonitor
    scrapeTimeout: ""
    # -- Set metricRelabelings for the endpoint of the serviceMonitor
    metricRelabelings: []
    # -- Set relabelings for the endpoint of the serviceMonitor
    relabelings: []
    #- action: replace
    #  regex: (.+)
    #  replacement: $1
    #  sourceLabels:
    #  - __meta_kubernetes_pod_name
    #  targetLabel: member
    #

alerts:
  # -- Enable alerts for Alertmanager
  enabled: false
  # -- Install the Alerts into a different Namespace, as the monitoring stack one (default: the release one)
  namespace: ''
  # -- Assign additional labels according to Prometheus' Alerts matching labels
  labels: {}
  # -- Assign additional Annotations
  annotations: {}
  # -- The rules for alerts
  rules: []
  #  - alert: etcdNoLeader
  #    annotations:
  #      message: 'etcd cluster: member {{ $labels.instance }} has no leader.'
  #    expr: count(etcd_server_has_leader{job=~".*etcd.*"}) == 0
  #    for: 1m
  #    labels:
  #      severity: critical
  #  - alert: EtcdDataBaseSize
  #    annotations:
  #      message: 'etcd cluster: "member {{ $labels.instance }} db has almost exceeded 8GB".'
  #    expr: |-
  #      etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"} >= 8589934592
  #    for: 15m
  #    labels:
  #      severity: critical
  #

jobs:
  # -- Kubernetes node selector rules for ancillary jobs
  nodeSelector:
    kubernetes.io/os: linux
  # -- Kubernetes node taints that the ancillary jobs would tolerate
  tolerations: []
  # -- Kubernetes affinity rules to apply to ancillary jobs
  affinity: {}
  # -- addional images to use for ancillary jobs
  cfssl:
    image: cfssl/cfssl
    tag: ""
  kubectl:
    image: clastix/kubectl
    tag: ""
  etcd:
    image: quay.io/coreos/etcd
    tag: v3.5.6 # latest container image with shell available!
    pullPolicy: IfNotPresent

selfSignedCertificates:
  # -- Enables the generation of self-signed certificates for etcd using the cfssl, and kubectl jobs.
  enabled: true

certManager:
  # -- Enable CertManager for etcd certificates
  enabled: false
  # -- CertManager Issuer to use for the etcd certificates
  issuerRef: {}
  #  name: kamaji-etcd-issuer
  #  kind: ClusterIssuer
  #  group: cert-manager.io
  ca:
    create: true
    nameOverride: ""
    # -- CertManager etcd CA validity
    validity: 87600h # 10 years
  serverCert:
    create: true
    nameOverride: ""
    additionalDnsNames: []
  peerCert:
    create: true
    nameOverride: ""
    additionalDnsNames: []
  clientCert:
    create: true
    nameOverride: ""
